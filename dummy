import { spawn } from "child_process";
import {
  Stagehand,
  LLMClient,
  CreateChatCompletionOptions,
  LLMResponse,
  AvailableModel,
} from "@browserbasehq/stagehand";
import https from "https";

/* -----------------------------------------------------
 * ðŸ§  Custom LLM Client
 * Extends Stagehandâ€™s LLMClient to support a custom API endpoint.
 * ----------------------------------------------------- */
class CustomLLMClient extends LLMClient {
  type = "custom";
  private apiEndpoint: string;
  private apiKey: string;
  private actualModelName: string;

  constructor({
    modelName = "gpt-4o" as AvailableModel,
    apiEndpoint,
    apiKey,
    actualModelName = "gpt-4.1-2025-04-14-eastus-dz",
  }: {
    modelName?: AvailableModel;
    apiEndpoint: string;
    apiKey: string;
    actualModelName?: string;
  }) {
    super(modelName);
    this.apiEndpoint = apiEndpoint;
    this.apiKey = apiKey;
    this.actualModelName = actualModelName;
    this.hasVision = false;
  }

  async createChatCompletion<T = LLMResponse>(
    options: CreateChatCompletionOptions,
    logger: (log: { category: string; level: number; message: string }) => void,
    retries = 3
  ): Promise<T> {
    const { messages, temperature, maxTokens, tools, tool_choice } = options;

    try {
      // Build request payload
      const requestPayload: Record<string, any> = {
        model: this.actualModelName,
        messages: messages.map((msg) => ({
          role: msg.role,
          content:
            typeof msg.content === "string"
              ? msg.content
              : JSON.stringify(msg.content),
        })),
        temperature,
        max_tokens: maxTokens,
      };

      // Handle tool definitions
      if (tools?.length) {
        requestPayload.tools = tools.map((tool) =>
          tool.function
            ? tool
            : {
                type: tool.type || "function",
                function: {
                  name: tool.name,
                  description: tool.description,
                  parameters: tool.parameters,
                },
              }
        );
      }

      if (tool_choice) {
        requestPayload.tool_choice = tool_choice;
      }

      // HTTPS request setup
      const requestBody = JSON.stringify(requestPayload);
      const url = new URL(this.apiEndpoint);

      const httpsOptions = {
        hostname: url.hostname,
        port: url.port || 443,
        path: url.pathname + url.search,
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${this.apiKey}`,
          "Content-Length": Buffer.byteLength(requestBody),
        },
        rejectUnauthorized: true, // âœ… Use false only for local self-signed certs
      };

      // Send request
      const data = await new Promise<any>((resolve, reject) => {
        const req = https.request(httpsOptions, (res) => {
          let responseData = "";
          res.on("data", (chunk) => (responseData += chunk));
          res.on("end", () => {
            if (res.statusCode && res.statusCode >= 200 && res.statusCode < 300) {
              try {
                resolve(JSON.parse(responseData));
              } catch {
                reject(new Error(`Failed to parse response: ${responseData}`));
              }
            } else {
              reject(
                new Error(
                  `API request failed (${res.statusCode}): ${responseData.slice(
                    0,
                    500
                  )}`
                )
              );
            }
          });
        });

        req.on("error", reject);
        req.write(requestBody);
        req.end();
      });

      // Normalize response
      const formattedResponse: LLMResponse = {
        id: data.id || `custom-${Date.now()}`,
        object: "chat.completion",
        created: data.created || Math.floor(Date.now() / 1000),
        model: this.actualModelName,
        choices: [
          {
            index: 0,
            message: {
              role: "assistant",
              content:
                data.choices?.[0]?.message?.content || data.content || "",
              tool_calls: data.choices?.[0]?.message?.tool_calls || [],
            },
            finish_reason: data.choices?.[0]?.finish_reason || "stop",
          },
        ],
        usage: {
          prompt_tokens: data.usage?.prompt_tokens || 0,
          completion_tokens: data.usage?.completion_tokens || 0,
          total_tokens: data.usage?.total_tokens || 0,
        },
      };

      return formattedResponse as T;
    } catch (error) {
      logger({
        category: "custom-llm",
        level: 0,
        message: `Error: ${
          error instanceof Error ? error.message : String(error)
        }`,
      });

      if (retries > 0) {
        console.warn(`Retrying... (${retries - 1} retries left)`);
        return this.createChatCompletion(options, logger, retries - 1);
      }
      throw error;
    }
  }
}

/* -----------------------------------------------------
 * ðŸ”‘ Fetch OAuth Config (via Python)
 * ----------------------------------------------------- */
async function fetchOAuthConfig(): Promise<{ apiKey: string; baseURL: string }> {
  return new Promise((resolve, reject) => {
    const pythonProcess = spawn("python", ["fetch_token.py"]);
    let output = "";
    let errorOutput = "";

    pythonProcess.stdout.on("data", (data) => (output += data.toString()));
    pythonProcess.stderr.on("data", (data) => (errorOutput += data.toString()));

    pythonProcess.on("close", (code) => {
      if (code !== 0) {
        return reject(new Error(`Python script failed: ${errorOutput}`));
      }

      try {
        const result = JSON.parse(output.trim());
        if (result.error) {
          reject(new Error(`OAuth error: ${result.error}`));
        } else {
          resolve(result);
        }
      } catch {
        reject(new Error(`Failed to parse OAuth response: ${output}`));
      }
    });
  });
}

/* -----------------------------------------------------
 * ðŸ§© Safe Action Wrapper (safeAct)
 * Adds retries, observation, and load waiting for reliability.
 * ----------------------------------------------------- */
export async function safeAct(
  stagehand: any,
  instruction: string,
  options: {
    retries?: number;
    timeout?: number;
    waitForLoadState?: "domcontentloaded" | "networkidle" | "load";
    waitBetweenRetries?: number;
  } = {}
) {
  const {
    retries = 3,
    timeout = 15000,
    waitForLoadState = "domcontentloaded",
    waitBetweenRetries = 2000,
  } = options;

  const page = stagehand.page;
  console.log(`ðŸ§­ safeAct: "${instruction}"`);

  await page.waitForLoadState(waitForLoadState);

  for (let attempt = 1; attempt <= retries; attempt++) {
    console.log(`ðŸ” Attempt ${attempt}: observing "${instruction}"...`);

    try {
      const [action] = await stagehand.observe(instruction, { timeout });

      if (action) {
        console.log(`âœ… Element found. Acting...`);
        await stagehand.act(action, { timeout });
        console.log(`ðŸŽ¯ Success: "${instruction}"`);
        return true;
      } else {
        console.warn(`âš ï¸ No actionable element found (attempt ${attempt})`);
      }
    } catch (err) {
      console.warn(`âš ï¸ Attempt ${attempt} failed: ${String(err)}`);
    }

    if (attempt < retries) {
      console.log(`â³ Retrying in ${waitBetweenRetries / 1000}s...`);
      await page.waitForTimeout(waitBetweenRetries);
    }
  }

  console.error(`âŒ Failed to execute: "${instruction}"`);
  return false;
}

/* -----------------------------------------------------
 * ðŸš€ Main Entry Point
 * ----------------------------------------------------- */
async function main() {
  console.log("ðŸš€ Starting Stagehand with OAuth...");

  try {
    // 1ï¸âƒ£ Fetch OAuth credentials
    const oauthConfig = await fetchOAuthConfig();

    // 2ï¸âƒ£ Initialize Custom LLM Client
    const rbcLLMClient = new CustomLLMClient({
      modelName: "gpt-4o",
      apiEndpoint: `${oauthConfig.baseURL}/chat/completions`,
      apiKey: oauthConfig.apiKey,
      actualModelName: "gpt-4.1-2025-04-14-eastus-dz",
    });

    // 3ï¸âƒ£ Create Stagehand instance
    const stagehand = new Stagehand({
      env: "LOCAL",
      llmClient: rbcLLMClient,
      verbose: 1,
      headless: false,
    });

    console.log("ðŸ§  Initializing Stagehand...");
    await stagehand.init();

    const page = stagehand.page;

    // 4ï¸âƒ£ Navigate and perform actions
    console.log("ðŸŒ Navigating to Google...");
    await page.goto("https://www.google.com");
    await page.waitForLoadState("domcontentloaded");

    await safeAct(stagehand, "accept cookies");
    await safeAct(stagehand, "type 'Stagehand LLM' into the search box");
    await safeAct(stagehand, "press the Enter key");

    console.log("âœ… Stagehand automation completed!");
  } catch (err) {
    console.error("âŒ Initialization error:", err);
  }
}

// Run main
main();
